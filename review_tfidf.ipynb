{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['정말', '밥을', '어제의', '맛이', '없었다', '먹을', '하면', '먹었다', '수', '나는', '밥은', '맛있는', '어떻게', '있을까']\n",
      "   정말  밥을  어제의  맛이  없었다  먹을  하면  먹었다  수  나는  밥은  맛있는  어떻게  있을까\n",
      "0   0   1    0   0    0   0   0    1  0   1   0    0    0    0\n",
      "1   1   0    1   1    1   0   0    0  0   0   1    0    0    0\n",
      "2   0   1    0   0    0   1   1    0  1   0   0    1    1    1\n",
      "          idf\n",
      "정말   0.405465\n",
      "밥을   0.000000\n",
      "어제의  0.405465\n",
      "맛이   0.405465\n",
      "없었다  0.405465\n",
      "먹을   0.405465\n",
      "하면   0.405465\n",
      "먹었다  0.405465\n",
      "수    0.405465\n",
      "나는   0.405465\n",
      "밥은   0.405465\n",
      "맛있는  0.405465\n",
      "어떻게  0.405465\n",
      "있을까  0.405465\n",
      "         정말   밥을       어제의        맛이       없었다        먹을        하면       먹었다  \\\n",
      "0  0.000000  0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.405465   \n",
      "1  0.405465  0.0  0.405465  0.405465  0.405465  0.000000  0.000000  0.000000   \n",
      "2  0.000000  0.0  0.000000  0.000000  0.000000  0.405465  0.405465  0.000000   \n",
      "\n",
      "          수        나는        밥은       맛있는       어떻게       있을까  \n",
      "0  0.000000  0.405465  0.000000  0.000000  0.000000  0.000000  \n",
      "1  0.000000  0.000000  0.405465  0.000000  0.000000  0.000000  \n",
      "2  0.405465  0.000000  0.000000  0.405465  0.405465  0.405465  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from math import log\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "doc_list = ['나는 밥을 먹었다', '어제의 밥은 정말 맛이 없었다', '어떻게 하면 맛있는 밥을 먹을 수 있을까']\n",
    "# 문서 전체의 단어들을 토큰화\n",
    "# token_list = Okt().morphs(' '.join(doc_list))\n",
    "token_list = []\n",
    "for doc in doc_list:\n",
    "    token_list += doc.split(\" \")\n",
    "# 중복 단어 제거\n",
    "token_list = list(set(token_list))\n",
    "print(token_list)\n",
    "# 빈도수 반환\n",
    "def tf(term, document): \n",
    "    return document.count(term)\n",
    "\n",
    "# log(n/(1+df(t)))\n",
    "def idf(term):\n",
    "    df = 0\n",
    "    for doc in doc_list:\n",
    "        # 각 문서마다 해당 단어가 있는지 확인\n",
    "        if term in doc:\n",
    "            df += 1\n",
    "    return log(len(doc_list)/(df+1))+1\n",
    "    \n",
    "    \n",
    "def tfidf(term, document):\n",
    "    return tf(term, document)*idf(term)\n",
    "\n",
    "dtm = []\n",
    "\n",
    "for doc in doc_list:\n",
    "    dtm.append([])\n",
    "    for token in token_list:\n",
    "        dtm[-1].append(tf(token,doc))\n",
    "\n",
    "# print(token_list)\n",
    "# print()\n",
    "# print(dtm)\n",
    "\n",
    "dtm_pd = pd.DataFrame(dtm,columns=token_list)\n",
    "print(dtm_pd)\n",
    "\n",
    "idf_list = []\n",
    "\n",
    "for token in token_list:\n",
    "    idf_list.append(idf(token))\n",
    "    \n",
    "\n",
    "idf_pd = pd.DataFrame(idf_list, columns=['idf'],index=token_list)\n",
    "print(idf_pd)\n",
    "\n",
    "tfidf_list = []\n",
    "\n",
    "for doc in doc_list:\n",
    "    tfidf_list.append([])\n",
    "    for token in token_list:\n",
    "        tfidf_list[-1].append(tfidf(token,doc))\n",
    "        \n",
    "tfidf_pd = pd.DataFrame(tfidf_list, columns=token_list)\n",
    "print(tfidf_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-6a260527cc33>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# 문서 전체의 단어들을 토큰화\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mtoken_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdoc_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;31m# 중복 단어 제거\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mtoken_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from math import log\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "doc_list = ['나는 밥을 먹었다', '어제의 밥은 정말 맛이 없었다', '어떻게 하면 맛있는 밥을 먹을 수 있을까']\n",
    "\n",
    "# 문서 전체의 단어들을 토큰화\n",
    "token_list = doc_list.split()\n",
    "# 중복 단어 제거\n",
    "token_list = list(set(token_list))\n",
    "\n",
    "# 빈도수 반환\n",
    "def tf(term, document): \n",
    "    return document.count(term)\n",
    "\n",
    "# log(n/(1+df(t)))\n",
    "def idf(term):\n",
    "    df = 0\n",
    "    for doc in doc_list:\n",
    "        # 각 문서마다 해당 단어가 있는지 확인\n",
    "        if term in doc:\n",
    "            df += 1\n",
    "    return log(len(doc_list)/(df+1))+1\n",
    "    \n",
    "    \n",
    "def tfidf(term, document):\n",
    "    return tf(term, document)*idf(term)\n",
    "\n",
    "dtm = []\n",
    "\n",
    "for doc in doc_list:\n",
    "    dtm.append([])\n",
    "    for token in token_list:\n",
    "        dtm[-1].append(tf(token,doc))\n",
    "\n",
    "# print(token_list)\n",
    "# print()\n",
    "# print(dtm)\n",
    "\n",
    "dtm_pd = pd.DataFrame(dtm,columns=token_list)\n",
    "print(dtm_pd)\n",
    "\n",
    "idf_list = []\n",
    "\n",
    "for token in token_list:\n",
    "    idf_list.append(idf(token))\n",
    "    \n",
    "\n",
    "idf_pd = pd.DataFrame(idf_list, columns=['idf'],index=token_list)\n",
    "print(idf_pd)\n",
    "\n",
    "tfidf_list = []\n",
    "\n",
    "for doc in doc_list:\n",
    "    tfidf_list.append([])\n",
    "    for token in token_list:\n",
    "        tfidf_list[-1].append(tfidf(token,doc))\n",
    "        \n",
    "tfidf_pd = pd.DataFrame(tfidf_list, columns=token_list)\n",
    "print(tfidf_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
