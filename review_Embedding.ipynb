{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "one-hot vector의 길이가 커지면 표현하기 어려워지므로 밀집 표현을 통해 차원을 줄인다.\n",
    "ex) [0.01, 0.2, 0.02, 0.06, 0.7, 0.01]처럼 단순 0,1이 아닌 실수값들을 가진다.\n",
    "밀집 표현에서 단어의 의미를 차원에 분산해서 표현하여 단어 간의 유사도를 계산할 수 있다."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "워드 임베딩: Word2Vec에는 2가지 방식이 있다.\n",
    "1: CBOW: 빈칸 문제와 같은 원리, 주변 단어로 해당 단어를 예측한다. (앞 뒤로 n개의 단어를 보고 예측)\n",
    "\n",
    "원 핫 벡터를 밀집 표현으로 바꿀때는 가중치 행렬을 곱하고 밀집 표현을 토대로 다시 원래 원-핫 벡터를 뽑아낸다\n",
    "(4개의 평균 * 가중치 = 결과값 -> softmax함수로 합이 1이 되도록 만듬 or cross-entropy 손실 함수로 손실을 최소화).\n",
    "\n",
    "2: Skip-Gram: 중심 단어로 앞 뒤 n개의 주변 단어를 예측한다.\n",
    "한 개의 중심 단어 원-핫 벡터를 밀집 표현으로 바꿀 때, 가중치를 곱하고 밀집 표현을 토대로 다시 원래 원-핫 벡터를 뽑아낸다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMDB 데이터로 Word2Vec을 만드는 것을 보고, 네이버 영화 리뷰로 Word2Vec 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "import csv\n",
    "import re\n",
    "\n",
    "def open_csv():\n",
    "    #     open csv\n",
    "    f = open('IMDBDataset.csv','r',encoding = 'utf-8')\n",
    "    csvreader = csv.reader(f)\n",
    "    \n",
    "    doc_list = []\n",
    "    \n",
    "    next(csvreader)\n",
    "    for f in csvreader:\n",
    "        line = re.compile(\"[^\\w]\").sub(' ', f[0].lower())\n",
    "        doc_list.append(line.split())\n",
    "        \n",
    "    return doc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['one', 'of', 'the', 'other', 'reviewers', 'has', 'mentioned', 'that', 'after', 'watching', 'just', '1', 'oz', 'episode', 'you', 'll', 'be', 'hooked', 'they', 'are', 'right', 'as', 'this', 'is', 'exactly', 'what', 'happened', 'with', 'me', 'br', 'br', 'the', 'first', 'thing', 'that', 'struck', 'me', 'about', 'oz', 'was', 'its', 'brutality', 'and', 'unflinching', 'scenes', 'of', 'violence', 'which', 'set', 'in', 'right', 'from', 'the', 'word', 'go', 'trust', 'me', 'this', 'is', 'not', 'a', 'show', 'for', 'the', 'faint', 'hearted', 'or', 'timid', 'this', 'show', 'pulls', 'no', 'punches', 'with', 'regards', 'to', 'drugs', 'sex', 'or', 'violence', 'its', 'is', 'hardcore', 'in', 'the', 'classic', 'use', 'of', 'the', 'word', 'br', 'br', 'it', 'is', 'called', 'oz', 'as', 'that', 'is', 'the', 'nickname', 'given', 'to', 'the', 'oswald', 'maximum', 'security', 'state', 'penitentary', 'it', 'focuses', 'mainly', 'on', 'emerald', 'city', 'an', 'experimental', 'section', 'of', 'the', 'prison', 'where', 'all', 'the', 'cells', 'have', 'glass', 'fronts', 'and', 'face', 'inwards', 'so', 'privacy', 'is', 'not', 'high', 'on', 'the', 'agenda', 'em', 'city', 'is', 'home', 'to', 'many', 'aryans', 'muslims', 'gangstas', 'latinos', 'christians', 'italians', 'irish', 'and', 'more', 'so', 'scuffles', 'death', 'stares', 'dodgy', 'dealings', 'and', 'shady', 'agreements', 'are', 'never', 'far', 'away', 'br', 'br', 'i', 'would', 'say', 'the', 'main', 'appeal', 'of', 'the', 'show', 'is', 'due', 'to', 'the', 'fact', 'that', 'it', 'goes', 'where', 'other', 'shows', 'wouldn', 't', 'dare', 'forget', 'pretty', 'pictures', 'painted', 'for', 'mainstream', 'audiences', 'forget', 'charm', 'forget', 'romance', 'oz', 'doesn', 't', 'mess', 'around', 'the', 'first', 'episode', 'i', 'ever', 'saw', 'struck', 'me', 'as', 'so', 'nasty', 'it', 'was', 'surreal', 'i', 'couldn', 't', 'say', 'i', 'was', 'ready', 'for', 'it', 'but', 'as', 'i', 'watched', 'more', 'i', 'developed', 'a', 'taste', 'for', 'oz', 'and', 'got', 'accustomed', 'to', 'the', 'high', 'levels', 'of', 'graphic', 'violence', 'not', 'just', 'violence', 'but', 'injustice', 'crooked', 'guards', 'who', 'll', 'be', 'sold', 'out', 'for', 'a', 'nickel', 'inmates', 'who', 'll', 'kill', 'on', 'order', 'and', 'get', 'away', 'with', 'it', 'well', 'mannered', 'middle', 'class', 'inmates', 'being', 'turned', 'into', 'prison', 'bitches', 'due', 'to', 'their', 'lack', 'of', 'street', 'skills', 'or', 'prison', 'experience', 'watching', 'oz', 'you', 'may', 'become', 'comfortable', 'with', 'what', 'is', 'uncomfortable', 'viewing', 'thats', 'if', 'you', 'can', 'get', 'in', 'touch', 'with', 'your', 'darker', 'side']]\n"
     ]
    }
   ],
   "source": [
    "doc_list = open_csv()\n",
    "print(doc_list[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('woman', 0.7532391548156738), ('guy', 0.7000030875205994), ('person', 0.6565152406692505), ('boy', 0.6371250748634338), ('lady', 0.6186240315437317), ('psychopath', 0.5865307450294495), ('soldier', 0.5848601460456848), ('doctor', 0.574589729309082), ('girl', 0.5574882626533508), ('priest', 0.5469307899475098)]\n"
     ]
    }
   ],
   "source": [
    "# window size= 왼쪽 오른쪽 n개의 단어를 보겠다.\n",
    "model = Word2Vec(sentences=doc_list, size=100, window=3, min_count=3, workers=4, sg=0)\n",
    "model_result = model.wv.most_similar(\"man\")\n",
    "print(model_result)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Word2 Vec 인자:\n",
    "sencences: 문장들\n",
    "size: 임베딩 벡터의 크기\n",
    "window: 고려할 앞/뒤 단어의 개수\n",
    "min_count: 최소 단어의 길이\n",
    "workers: 사용할 프로세서의 수\n",
    "sg:0 = cbow, 1:skipgram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장\n",
    "model.wv.save_word2vec_format('imdb_w2v')\n",
    "# 불러오기\n",
    "imdb_model = KeyedVectors.load_word2vec_format('imdb_w2v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 네이버 영화 리뷰로 w2v 모델 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-8d7589530533>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdoc_list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[0mdoc_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen_txt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-22-8d7589530533>\u001b[0m in \u001b[0;36mopen_txt\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mstc_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\t'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mstc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKkma\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmorphs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstc_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0msw_removed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python38\\lib\\site-packages\\konlpy\\tag\\_kkma.py\u001b[0m in \u001b[0;36mmorphs\u001b[1;34m(self, phrase)\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[1;34m\"\"\"Parse phrase to morphemes.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mphrase\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mphrase\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python38\\lib\\site-packages\\konlpy\\tag\\_kkma.py\u001b[0m in \u001b[0;36mpos\u001b[1;34m(self, phrase, flatten, join)\u001b[0m\n\u001b[0;32m     64\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mflatten\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m                     \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meojeol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m                         \u001b[0mmorpheme\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meojeol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m                         \u001b[1;32mif\u001b[0m \u001b[0mjoin\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m                             \u001b[0mmorphemes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmorpheme\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'/'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmorpheme\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetTag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from konlpy.tag import Kkma\n",
    "import csv\n",
    "import re\n",
    "\n",
    "def open_txt():\n",
    "    f = open(\"../data/ratings.txt\",'r',encoding='utf-8')\n",
    "    \n",
    "    doc_list = []\n",
    "    \n",
    "    sw = ['을','를','이','가']\n",
    "    \n",
    "    for line in f:\n",
    "        stc_list = line.split('\\t')\n",
    "        stc = Kkma().morphs(stc_list[1])\n",
    "        \n",
    "        sw_removed = []\n",
    "        \n",
    "        stc = stc_list[1].split()\n",
    "        for i in stc:\n",
    "            if i not in sw:\n",
    "                sw_removed.append(i)\n",
    "                \n",
    "        doc_list.append(sw_removed)\n",
    "        \n",
    "    return doc_list\n",
    "\n",
    "doc_list = open_txt()\n",
    "print(doc_list[:3])\n",
    "\n",
    "model = Word2Vec(sentences=doc_list, size=50, window=4, min_count=3, workers=4, sg=1)\n",
    "model_result = model.wv.most_similar('남자')\n",
    "print(model_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
