{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "review_glove.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePYy2dPaQzX-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "import re\n",
        "\n",
        "def open_csv():\n",
        "    # csv파일을 연다!\n",
        "    f = open('IMDBDataset.csv', 'r', encoding='utf-8')\n",
        "    csvreader = csv.reader(f)\n",
        "    \n",
        "    doc_list = []\n",
        "\n",
        "    next(csvreader)\n",
        "    for f in csvreader:\n",
        "        line = re.compile(\"[^\\w]\").sub(' ', f[0].lower())\n",
        "        doc_list.append(line.split())\n",
        "\n",
        "    return doc_list\n",
        "\n",
        "doc_list = open_csv()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbwa5Xed-TXi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "outputId": "e98d9b77-4557-4852-c633-b22e55418a64"
      },
      "source": [
        "!pip install glove_python"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting glove_python\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3e/79/7e7e548dd9dcb741935d031117f4bed133276c2a047aadad42f1552d1771/glove_python-0.1.0.tar.gz (263kB)\n",
            "\r\u001b[K     |█▎                              | 10kB 16.7MB/s eta 0:00:01\r\u001b[K     |██▌                             | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |███▊                            | 30kB 2.3MB/s eta 0:00:01\r\u001b[K     |█████                           | 40kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 51kB 2.0MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 61kB 2.3MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 71kB 2.5MB/s eta 0:00:01\r\u001b[K     |██████████                      | 81kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 92kB 2.9MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 102kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 112kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 122kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 133kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 143kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 153kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 163kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 174kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 184kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 194kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 204kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 215kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 225kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 235kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 245kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 256kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 266kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from glove_python) (1.18.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from glove_python) (1.4.1)\n",
            "Building wheels for collected packages: glove-python\n",
            "  Building wheel for glove-python (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for glove-python: filename=glove_python-0.1.0-cp36-cp36m-linux_x86_64.whl size=700247 sha256=bbe84c5bad242edda51e2fc424484cb101eb034118c03ee9adc69ce934f5bcc9\n",
            "  Stored in directory: /root/.cache/pip/wheels/88/4b/6d/10c0d2ad32c9d9d68beec9694a6f0b6e83ab1662a90a089a4b\n",
            "Successfully built glove-python\n",
            "Installing collected packages: glove-python\n",
            "Successfully installed glove-python-0.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ph1A01_TpIt3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "outputId": "2480d8db-7359-4669-a60e-13b008db397b"
      },
      "source": [
        "from glove import Corpus, Glove\n",
        "\n",
        "corpus = Corpus()\n",
        "# 같이 등장한다는 것의 기준을 5로 지정\n",
        "corpus.fit(doc_list, window=5)\n",
        "\n",
        "# 경사하강법을 사용하기 때문에 학습률을 설정, 아웃풋 벡터의 차원은 100\n",
        "glove = Glove(no_components=100, learning_rate=0.05)\n",
        "# 쓰레드 개수는 4개, 훈련 횟수는 20번, verbose 설명\n",
        "glove.fit(corpus.matrix, epochs=20, no_threads=4, verbose=True)\n",
        "# 유사도 검색을 위해서 행렬의 index 정보를 입력\n",
        "glove.add_dictionary(corpus.dictionary)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Performing 20 training epochs with 4 threads\n",
            "Epoch 0\n",
            "Epoch 1\n",
            "Epoch 2\n",
            "Epoch 3\n",
            "Epoch 4\n",
            "Epoch 5\n",
            "Epoch 6\n",
            "Epoch 7\n",
            "Epoch 8\n",
            "Epoch 9\n",
            "Epoch 10\n",
            "Epoch 11\n",
            "Epoch 12\n",
            "Epoch 13\n",
            "Epoch 14\n",
            "Epoch 15\n",
            "Epoch 16\n",
            "Epoch 17\n",
            "Epoch 18\n",
            "Epoch 19\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgP3HEi97Ws-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "fccefdf4-6074-4e75-d7d0-61519a8e0a0c"
      },
      "source": [
        "glove_result = glove.most_similar('dog')\n",
        "print(glove_result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('spot', 0.9534997404936615), ('road', 0.9463646793520095), ('basic', 0.9407350386932332), ('creature', 0.9382841166227128)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lROXox7B-u1f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 575
        },
        "outputId": "c66d71f4-043f-4df7-dca5-5241ec4bb5ed"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "token = Tokenizer()\n",
        "text = \"Tensorflow tokenizer is good\"\n",
        "\n",
        "# 리스트 형태로 넣어주어야 제대로 토큰화가 된다 (아니면 한글자씩 자름)\n",
        "# fit 한 것을 토대로 추후에 sequence를 생성한다.\n",
        "token.fit_on_texts([text])\n",
        "\n",
        "# 단어 집합\n",
        "print(token.word_index)\n",
        "# 단어 빈도\n",
        "print(token.word_counts)\n",
        "\n",
        "# 정수 인코딩\n",
        "text_2 = \"Tensorflow tokenizer is not good good\"\n",
        "int_encoding = token.texts_to_sequences([text_2])\n",
        "print(int_encoding)\n",
        "\n",
        "# 행렬화\n",
        "text_list = ['안녕하세요 감사해요 잘있어요 다시 만나요','오늘도 기분 좋은 하루','안녕하세요 오늘도 감사해요']\n",
        "token.fit_on_texts(text_list)\n",
        "\n",
        "# count는 dtm, binary는 존재 여부, freq는 빈도율, tfidf는 tfidf\n",
        "print(\"tfidf\")\n",
        "matrix1 = token.texts_to_matrix(text_list, mode='tfidf')\n",
        "print(matrix1)\n",
        "print(\"dtm\")\n",
        "matrix2 = token.texts_to_matrix(text_list, mode='count')\n",
        "print(matrix2)\n",
        "print(\"존재여부\")\n",
        "matrix3 = token.texts_to_matrix(text_list, mode='binary')\n",
        "print(matrix3)\n",
        "print(\"빈도\")\n",
        "matrix4 = token.texts_to_matrix(text_list, mode='freq')\n",
        "print(matrix4)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'tensorflow': 1, 'tokenizer': 2, 'is': 3, 'good': 4}\n",
            "OrderedDict([('tensorflow', 1), ('tokenizer', 1), ('is', 1), ('good', 1)])\n",
            "[[1, 2, 3, 4, 4]]\n",
            "tfidf\n",
            "[[0.         0.84729786 0.84729786 0.         0.         0.\n",
            "  0.         0.         1.09861229 1.09861229 1.09861229 0.\n",
            "  0.         0.        ]\n",
            " [0.         0.         0.         0.84729786 0.         0.\n",
            "  0.         0.         0.         0.         0.         1.09861229\n",
            "  1.09861229 1.09861229]\n",
            " [0.         0.84729786 0.84729786 0.84729786 0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.        ]]\n",
            "dtm\n",
            "[[0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]\n",
            " [0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "존재여부\n",
            "[[0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]\n",
            " [0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "빈도\n",
            "[[0.         0.2        0.2        0.         0.         0.\n",
            "  0.         0.         0.2        0.2        0.2        0.\n",
            "  0.         0.        ]\n",
            " [0.         0.         0.         0.25       0.         0.\n",
            "  0.         0.         0.         0.         0.         0.25\n",
            "  0.25       0.25      ]\n",
            " [0.         0.33333333 0.33333333 0.33333333 0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7n582zp4SRSo",
        "colab_type": "text"
      },
      "source": [
        "### 모델 사용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NL9ZKQt7WSUW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "token = Tokenizer()\n",
        "text = \"Keras tokenizer is good\"\n",
        "word_count = len(text)\n",
        "token.fit_on_texts([text])"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqrJt9BCSTWp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "f = open('glove.6B.50d.txt', encoding='utf-8')\n",
        "\n",
        "word_vectors = {}\n",
        "\n",
        "for line in f:\n",
        "    word_vector = line.split()\n",
        "    word_vectors[word_vector[0]] = word_vector[1:]\n",
        "\n",
        "matrix = np.zeros((word_count, 300))\n",
        "# ex) 4*50 행렬이 만들어짐\n",
        "\n",
        "\n",
        "# 없는 단어에 대한 예외처리\n",
        "for word, i in token.word_index.items():\n",
        "    matrix[i] = np.array(word_vectors['word'])\n",
        "\n",
        "# 실제로 사용\n",
        "model = Sequential()\n",
        "model.add(Embedding(word_count, 50, weights=[matrix]))\n",
        "# glove로 임베딩된 벡터 값들을 가지고 실제로 모델에서 값들을 사용하는 임베딩 층을 만든다.\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oaqFJK_a0DI",
        "colab_type": "text"
      },
      "source": [
        "### 구글에서 만든 word2Vec 모델은 bin 형태로 만들어져 있으므로 사용가능"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pT3IMQeRV8Db",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim.models import KeyedVectors\n",
        "\n",
        "word2vec_model = KeyedVectors.load_word2vec_format('파일이름.bin.gz', binary=True)\n",
        "\n",
        "matrix = np.zeros((word_count, 300))\n",
        "# ex) 4*50 행렬이 만들어짐\n",
        "\n",
        "\n",
        "# 없는 단어에 대한 예외처리\n",
        "for word, i in token.word_index.items():\n",
        "    matrix[i] = np.array(word_vectors['word'])\n",
        "\n",
        "# 실제로 사용\n",
        "model = Sequential()\n",
        "model.add(Embedding(word_count, 50, weights=[matrix]))\n",
        "# glove로 임베딩된 벡터 값들을 가지고 실제로 모델에서 값들을 사용하는 임베딩 층을 만든다.\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}